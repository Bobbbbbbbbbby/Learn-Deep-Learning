{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let Neural Network Learn\n",
    "\n",
    "Machine Learning:\n",
    "* Extract characteristic quantities, which can present the essential character of the original pictures\n",
    "* Use machine learning techniques to learn the patterns of the pictures.\n",
    "\n",
    "But neural network can learn the characteristic of a picture directly, does not need people to come up with the idea about what should be used to present the characteristics.\n",
    "\n",
    "One of the main boons of the neural network is that it has the same procedure to handle every kind of problems.\n",
    "\n",
    "Training data is to train the network, testing data is to test those data that is not used in training. The final goal of training is to generalize, making the network work for all the input data.\n",
    "\n",
    "Over fitting: a state that a network works extremely well for a specific data set but not good for other data sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Loss function is a metric to represent how bad the network is. To be more specific, in what extent the network is not able to predict the training data set.\n",
    "\n",
    "### Mean Squared Error\n",
    "$$\n",
    "E = \\frac{1}{2}\\sum\\limits_k(y_k - t_k)^2\n",
    "$$\n",
    "* $y_k$ is the output of the network\n",
    "* $t_k$ is the oversight data(data from the training data set)\n",
    "* $k$ is the dimension count of the data\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high accuracy:\n",
      "<class 'numpy.float64'>\n",
      "0.09750000000000003\n",
      "low accuracy:\n",
      "<class 'numpy.float64'>\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "def mean_squared_error(y: np.ndarray, t: np.ndarray) -> np.float32:\n",
    "    diff: np.ndarray = y - t\n",
    "    diff *= diff\n",
    "    return np.sum(diff) / 2\n",
    "\n",
    "result: np.float32 = mean_squared_error(y1, t)\n",
    "print(\"high accuracy:\")\n",
    "print(type(result))\n",
    "print(result)\n",
    "\n",
    "result: np.float32 = mean_squared_error(y2, t)\n",
    "print(\"low accuracy:\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entrophy Error\n",
    "$$\n",
    "E = -\\sum\\limits_k t_k\\log y_k\n",
    "$$\n",
    "\n",
    "Because the $t_k$ only have one $t_i$ is 1, others are all 0. Therefore, $E$ is only calculating the $\\log$ of only one $y_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high accuracy:\n",
      "<class 'numpy.float64'>\n",
      "0.510825457099338\n",
      "low accuracy:\n",
      "<class 'numpy.float64'>\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "def cross_entrophy_error(y: np.ndarray, t: np.ndarray) -> np.float32:\n",
    "    y += 1e-7\n",
    "    y = np.log(y)\n",
    "    sum: np.float32 = np.sum(t * y)\n",
    "    return -sum\n",
    "\n",
    "result: np.float32 = cross_entrophy_error(y1, t)\n",
    "print(\"high accuracy:\")\n",
    "print(type(result))\n",
    "print(result)\n",
    "\n",
    "result: np.float32 = cross_entrophy_error(y2, t)\n",
    "print(\"low accuracy:\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch training\n",
    "We cannot just calculate every loss, add up and normalize. If the dataset is too big, do training for a time can take a long time.\n",
    "\n",
    "The training of a neural network will choose a mini-batch, and let it learn based on the mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist mini-batch\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from my_mnist import load_mnist\n",
    "\n",
    "x_train: np.ndarray\n",
    "t_train: np.ndarray\n",
    "x_test: np.ndarray\n",
    "t_test: np.ndarray\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(one_hot_label=True)\n",
    "\n",
    "# do a mini-batch of size of 10\n",
    "batch_size: np.int32 = 10\n",
    "mask: np.ndarray = np.random.choice(len(x_train), batch_size)\n",
    "x_batch: np.ndarray = x_train[mask]\n",
    "t_batch: np.ndarray = t_train[mask]\n",
    "# this way to visit the elements in array is that only the True idx will be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (10,784) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m\n\u001b[0;32m     19\u001b[0m (y, t) \u001b[39m=\u001b[39m mini_batch(\u001b[39m10\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m result \u001b[39m=\u001b[39m batch_cross_entrophy_error(y, t)\n",
      "Cell \u001b[1;32mIn[83], line 15\u001b[0m, in \u001b[0;36mbatch_cross_entrophy_error\u001b[1;34m(y, t)\u001b[0m\n\u001b[0;32m     13\u001b[0m y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-7\u001b[39m\n\u001b[0;32m     14\u001b[0m logy: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(y)\n\u001b[1;32m---> 15\u001b[0m \u001b[39msum\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(t \u001b[39m*\u001b[39m logy)\n\u001b[0;32m     16\u001b[0m \u001b[39msum\u001b[39m \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m batch_size \u001b[39m# normalize\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39msum\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (10,784) "
     ]
    }
   ],
   "source": [
    "# x_train, t_train, x_test, t_test have already been loaded previously\n",
    "def mini_batch(size: int):\n",
    "    mask:np.ndarray = np.random.choice(len(x_train), size)\n",
    "    x_batch:np.ndarray = x_train[mask]\n",
    "    t_batch:np.ndarray = t_train[mask]\n",
    "    return x_batch, t_batch\n",
    "\n",
    "# if t_train is in the form of one hot, cross entrophy is super easy\n",
    "def batch_cross_entrophy_error(y: np.ndarray, t: np.ndarray) -> np.float32:\n",
    "    batch_size: np.int32 = len(y)\n",
    "    print(f\"batch_size = {batch_size}\")\n",
    "    # shape of y and t is the same, therefore we do not neet to reshape and we can use the element wise operate directly\n",
    "    y += 1e-7\n",
    "    logy: np.ndarray = np.log(y)\n",
    "    sum = np.sum(t * logy)\n",
    "    sum /= batch_size # normalize\n",
    "    return -sum\n",
    "\n",
    "(y, t) = mini_batch(10)\n",
    "result = batch_cross_entrophy_error(y, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
