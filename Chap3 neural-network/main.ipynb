{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "In last chapter, I write about the mighty strength of a perceptron, as powerful as a CPU, but there is a bad news:\n",
    "\n",
    "I need to set the weight manually.\n",
    "\n",
    "Neural network is to solve this problem, which can learn the propriate weight from the data.\n",
    "\n",
    "This chapter will introduce the neural network, next chapter will show how to train a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Perceptron to Neural Network\n",
    "### Example of Neural Network\n",
    "The picture below shows a 2-layer neural network:\n",
    "\n",
    "<div align=\"center\"><img src=\"example1.png\" width=50% height=50%><div\\>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we call this as a *2-layer* network is that this network only has 2 layers of weight, which is the key of the network.\n",
    "\n",
    "First layer is called the **input layer**\n",
    "\n",
    "Second layer is called the **hidden layer**, this layer can have several sub-layer.\n",
    "\n",
    "Third layer is called the **output layer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the Perceptron\n",
    "The picture below shows the perceptron learned before:\n",
    "<div align=\"center\"><img src=\"example2.png\" height=30% width=30%></div>\n",
    "\n",
    "Although we use this model:\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = \n",
    "\\begin{cases}\n",
    "0 & (b + w_1x_1 + w_2x_2\\le 0)\\\\\n",
    "1 & (b + w_1x_1 + w_2x_2 > 0)\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$b$ is not showed in the picture of the perceptron.\n",
    "\n",
    "If we want to show $b$ in the picture, we can use a input 1 with width $b$ to present:\n",
    "<div align=\"center\"><img src=\"example3.png\" height=30% width=30%></div>\n",
    "\n",
    "Moreover, we can use a function to process $b+w_1x_1 + w_2x_2$ and get the new form:\n",
    "$$\n",
    "h(x) = \\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "1 & x > 0\n",
    "\\end{cases}\\\\\n",
    "y = h(b + w_1x_1 + w_2x_2)\n",
    "$$\n",
    "\n",
    "This $h$ is what we call *activation function*, which can be showed explicitly in the picture below:\n",
    "<div align=\"center\"><img src=\"example4.png\" height=40% width=40%></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "### sigmoid Function\n",
    "$$\n",
    "h(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### ReLU Function\n",
    "$$\n",
    "h(x) = \\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "x & x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Show the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def step_function(x):\n",
    "    y = x > 0                   # if x is an ndarray, y will be bool element-wise\n",
    "    return y.astype(np.int32)   # convert y element-wise to int\n",
    "\n",
    "# creating a function for ndarray is needed because the x and y for a plot is a ndarray\n",
    "# it is easy to understand the element-wise, because what has been showed before for the ndarray is all element-wise operation\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1) # set range of y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x) :\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def ReLU(x) :\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = ReLU(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Demension Array\n",
    "### Multipy Matrixes\n",
    "Use `np.dot` to multiply an m * n matrix with an n * m matrix\n",
    "\n",
    "Matrix multiplication can be used to implement the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "c = np.dot(a, b)\n",
    "print(f\"shape of c is:\\n{c.shape}\\n\")\n",
    "print(f\"c is:\\n{c}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a 3-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer, output to a\n",
    "# First form:\n",
    "x = np.array([1.0, 0.5])\n",
    "w = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "b = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "a = np.dot(x, w) + b\n",
    "a = sigmoid(a)\n",
    "\n",
    "print(a)\n",
    "# Second form:\n",
    "# x = np.array([1, 0.5, 0.7])       [b, x1, x2]\n",
    "# w = np.array([[0.1, 0.2, 0.3], [0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])     [wb, wx1, wx2]\n",
    "# a1 = np.dot(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second layer, output to a, only write one form:\n",
    "w = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "a = np.dot(a, w) + b\n",
    "a = sigmoid(a)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third layer, from second layer to the output layer\n",
    "w = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "a = np.dot(a, w) + b\n",
    "# in the output layer, the function we use is not the activation function, here we just use equal\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Way to Create a Network\n",
    "Use a function `init_network` to define a network\n",
    "\n",
    "Use the function `forward` to implement the procedure of the network\n",
    "\n",
    "*Forward means data go forward from the input layer to output layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network[\"W1\"] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network[\"b1\"] = np.array([0.1, 0.2, 0.3])\n",
    "    network[\"W2\"] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network[\"b2\"] = np.array([0.1, 0.2])\n",
    "    network[\"W3\"] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network[\"b3\"] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    a1 = sigmoid(a1)\n",
    "    a2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(a2)\n",
    "    a3 = np.dot(a2, W3) + b3\n",
    "    y = a3\n",
    "\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1, 1])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the Output Layer\n",
    "In short, regression problems use the equal function, as showed above; classification problems use the `softmax` function\n",
    "\n",
    "Just neglect the equal function, too trivial\n",
    "\n",
    "### `softmax` Function\n",
    "$$\n",
    "y_k = \\frac{e^{a_k}}{\\sum e^{a_i}}\n",
    "$$\n",
    "\n",
    "There are $n$ neurons in the output layer, the $y_k$ is the output of the k-th neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "def softmax(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.sum(np.exp(x))\n",
    "    y = a / b\n",
    "    return y\n",
    "\n",
    "y = softmax(np.array([0.3, 2.9, 4.0]))\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation showed above is right, but there is a problem: OVERFLOW! Because exp is super big\n",
    "\n",
    "To solve this problem, use this method:\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_k &= \\frac{e^{a_k}}{\\sum e^{a_i}}\\\\\n",
    "&=\\frac{C\\times e^{a_k}}{C\\times \\sum e^{a_i}}\\\\\n",
    "&=\\frac{e^{a_k + C'}}{\\sum e^{a_i + C'}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With this method, we can minus C' before doing the exponents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify softmax\n",
    "def mod_softmax(x):\n",
    "    a = np.exp(x - np.max(x))\n",
    "    b = np.sum(np.exp(x - np.max(x)))\n",
    "    y = a / b\n",
    "    return y\n",
    "\n",
    "y1 = softmax(np.array([1000, 1010, 1020]))\n",
    "y2 = mod_softmax(np.array([1000, 1010, 1020]))\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "# python will show that there are overflow in y1, but everything is ok in y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristic of `softmax`\n",
    "1. output is always between 0 and 1\n",
    "2. sum of output is always 1\n",
    "\n",
    "Therefore, softmax can be understood as a probability, and the neuron with the biggest output is the result we want."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Application: Handwrite Recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and import the mnist dataset\n",
    "from my_mnist import init_mnist\n",
    "\n",
    "init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show imgs\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from my_mnist import load_mnist\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)\n",
    "img = img.reshape(28, 28)\n",
    "#img_show(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Neural Network\n",
    "Input layer: because there are 784 pixels in a picture, there should be 784 neurons in the input layer.\n",
    "\n",
    "Output layer: because there are 10 different possible numbers, there should be 10 neurons in the output layer.\n",
    "\n",
    "According to the book, there are 50 neurons in the first hidden layer, there are 100 neurons in the second hidden layer.\n",
    "\n",
    "* `get_data()`\n",
    "  > get data from mnist dataset\n",
    "* `init_network()`\n",
    "  > get weight from the pretrained network, stored in file `sample_weight.pkl`\n",
    "* `predict()`\n",
    "  > from input layer to the output layer and get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    \n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "\n",
    "    a = np.dot(x, W1) + b1\n",
    "    a = sigmoid(a)\n",
    "    a = np.dot(a, W2) + b2\n",
    "    a = sigmoid(a)\n",
    "    a = np.dot(a, W3) + b3\n",
    "    y = softmax(a)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pickle can store the object used in the runtime into a file, which can be restored as the same as what is stored this time in the next time when I need to use.\n",
    "\n",
    "after defining these functions, we can use them to do the prediction and figure out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accurate_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    # argmax will return the index of the biggest argument\n",
    "    if p == t[i]:\n",
    "        accurate_cnt += 1;\n",
    "\n",
    "accuracy = accurate_cnt / len(x)\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "What if I want the network process 100 image at a time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accurate_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i : i + batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis = 1)\n",
    "    accurate_cnt += np.sum(p == t[i : i + batch_size])\n",
    "\n",
    "accuracy = accurate_cnt / len(x)\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `batch_size = 100`\n",
    "  > process 100 pictures at a time\n",
    "* `y_batch`\n",
    "  > element wise operation\n",
    "* `p`\n",
    "  > get max idx in every row, if axis = 0, will get max idx in every column\n",
    "* `accurate_cnt`\n",
    "  > == will generate a bool array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array(\n",
    "    [[0.1, 0.1, 0.2, 0.1],\n",
    "    [0.2, 0.8, 0.1, 0.3]]\n",
    ")\n",
    "print(np.argmax(test_array, axis = 0)) # 0 means x, every x get the max idx y\n",
    "print(np.argmax(test_array, axis = 1)) # 1 means y, every y get the max idx x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
