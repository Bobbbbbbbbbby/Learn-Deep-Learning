{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let Neural Network Learn\n",
    "\n",
    "Machine Learning:\n",
    "* Extract characteristic quantities, which can present the essential character of the original pictures\n",
    "* Use machine learning techniques to learn the patterns of the pictures.\n",
    "\n",
    "But neural network can learn the characteristic of a picture directly, does not need people to come up with the idea about what should be used to present the characteristics.\n",
    "\n",
    "One of the main boons of the neural network is that it has the same procedure to handle every kind of problems.\n",
    "\n",
    "Training data is to train the network, testing data is to test those data that is not used in training. The final goal of training is to generalize, making the network work for all the input data.\n",
    "\n",
    "Over fitting: a state that a network works extremely well for a specific data set but not good for other data sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Loss function is a metric to represent how bad the network is. To be more specific, in what extent the network is not able to predict the training data set.\n",
    "\n",
    "### Mean Squared Error\n",
    "$$\n",
    "E = \\frac{1}{2}\\sum\\limits_k(y_k - t_k)^2\n",
    "$$\n",
    "* $y_k$ is the output of the network\n",
    "* $t_k$ is the oversight data(data from the training data set)\n",
    "* $k$ is the dimension count of the data\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "def mean_squared_error(y: np.ndarray, t: np.ndarray) -> np.float64:\n",
    "    diff: np.ndarray = y - t\n",
    "    diff *= diff\n",
    "    return np.sum(diff) / 2\n",
    "\n",
    "result: np.float64 = mean_squared_error(y1, t)\n",
    "print(\"high accuracy:\")\n",
    "print(type(result))\n",
    "print(result)\n",
    "\n",
    "result: np.float64 = mean_squared_error(y2, t)\n",
    "print(\"low accuracy:\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entrophy Error\n",
    "$$\n",
    "E = -\\sum\\limits_k t_k\\log y_k\n",
    "$$\n",
    "\n",
    "Because the $t_k$ only have one $t_i$ is 1, others are all 0. Therefore, $E$ is only calculating the $\\log$ of only one $y_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entrophy_error(y: np.ndarray, t: np.ndarray) -> np.float64:\n",
    "    y += 1e-7\n",
    "    y = np.log(y)\n",
    "    sum: np.float64 = np.sum(t * y)\n",
    "    return -sum\n",
    "\n",
    "result: np.float64 = cross_entrophy_error(y1, t)\n",
    "print(\"high accuracy:\")\n",
    "print(type(result))\n",
    "print(result)\n",
    "\n",
    "result: np.float64 = cross_entrophy_error(y2, t)\n",
    "print(\"low accuracy:\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch training\n",
    "We cannot just calculate every loss, add up and normalize. If the dataset is too big, do training for a time can take a long time.\n",
    "\n",
    "The training of a neural network will choose a mini-batch, and let it learn based on the mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist mini-batch\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from my_mnist import load_mnist\n",
    "import my_func as func\n",
    "\n",
    "x_train: np.ndarray\n",
    "t_train: np.ndarray\n",
    "x_test: np.ndarray\n",
    "t_test: np.ndarray\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(one_hot_label=True)\n",
    "\n",
    "# do a mini-batch of size of 10\n",
    "batch_size: np.int32 = np.int32(10)\n",
    "mask: np.ndarray = np.random.choice(len(x_train), batch_size)\n",
    "x_batch: np.ndarray = x_train[mask]\n",
    "t_batch: np.ndarray = t_train[mask]\n",
    "# this way to visit the elements in array is that only the True idx will be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, t_train, x_test, t_test have already been loaded previously\n",
    "def mini_batch(size: int):\n",
    "    mask:np.ndarray = np.random.choice(len(x_train), size)\n",
    "    x_batch:np.ndarray = x_train[mask]\n",
    "    t_batch:np.ndarray = t_train[mask]\n",
    "    return x_batch, t_batch\n",
    "\n",
    "# if t_train is in the form of one hot, cross entrophy is super easy\n",
    "def batch_cross_entrophy_error(y: np.ndarray, t: np.ndarray) -> np.float64:\n",
    "    batch_size = len(y)\n",
    "    print(f\"batch_size = {batch_size}\")\n",
    "    # shape of y and t is the same, therefore we do not neet to reshape and we can use the element wise operate directly\n",
    "    y += 1e-7\n",
    "    logy: np.ndarray = np.log(y)\n",
    "    sum = np.sum(t * logy)\n",
    "    sum /= batch_size # normalize\n",
    "    return -sum\n",
    "\n",
    "network = func.init_network()\n",
    "\n",
    "(y, t) = mini_batch(10)\n",
    "\n",
    "y = func.predict(network, y)\n",
    "# loop below is to normalize\n",
    "for x in y:\n",
    "    x /= np.sum(x)\n",
    "\n",
    "result = batch_cross_entrophy_error(y, t)\n",
    "print(result)\n",
    "\n",
    "# the non-one-hot form read the book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "Using the gradience to decide how to change the coefficience to reduce the loss\n",
    "\n",
    "### Implement Differentiation\n",
    "Use the center differential:\n",
    "$$\n",
    "f'(x) = \\lim\\limits_{h\\to 0}\\frac{f(x + h) - f(x - h)}{2h}\n",
    "$$\n",
    "\n",
    "Rather than the forward differential:\n",
    "$$\n",
    "f'(x) = \\lim\\limits_{h\\to 0}\\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "Because when h can not be infinitesimal, center differential is closer to the $f'(x)$ than the forward differential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / 2 * h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example:\n",
    "$$\n",
    "y = 0.01x^2 + 0.1x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return 0.01 * x * x + 0.1 * x\n",
    "\n",
    "print(numerical_diff(function, 5))\n",
    "print(numerical_diff(function, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivative\n",
    "Consider function:\n",
    "$$\n",
    "f(x_0, x_1) = x_0^2 + x_1^2\n",
    "$$\n",
    "\n",
    "Partial derivative of $x_1$ will show how fast the value will change in the direction of $x_1$\n",
    "\n",
    "Now want to know how to calculate $\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "To calculate $\\frac{\\partial f}{\\partial x_0}$ at $(3, 4)$, we can create a new function:\n",
    "$$\n",
    "f(x_0) = x_0 ^2 + 4 ^ 2\n",
    "$$\n",
    "and use the `numerical diff` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(x0, x1):\n",
    "    return x0 * x0 + x1 * x1\n",
    "\n",
    "def function1_mod(x0):\n",
    "    return x0 * x0 + 4 * 4\n",
    "\n",
    "print(f\"f'(3, 4) for x0 is {numerical_diff(function1_mod, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Easy to understand the function below:\n",
    "* create a `grad`\n",
    "* for each $x_i$, calculate `grad[i]`\n",
    "* return `grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(len(x)):\n",
    "        temp_val = x[idx]\n",
    "\n",
    "        x[idx] += h\n",
    "        f_plus_h = f(x)\n",
    "        x[idx] = temp_val - h\n",
    "        f_sub_h = f(x)\n",
    "\n",
    "        grad[idx] = (f_plus_h - f_sub_h) / (2 * h)\n",
    "        x[idx] = temp_val\n",
    "    return grad\n",
    "\n",
    "def function2(x):\n",
    "    return x[0] * x[0] + x[1] * x[1]\n",
    "\n",
    "print(f\"function2 grad at (3, 4) is {numerical_grad(function2, np.array([3.0, 4.0]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector of the gradient will point to the direction in which the value of the function will decrease\n",
    "\n",
    "Therefore, if we can get the loss function and calculate the gradient of it, we will know how to reduce the loss\n",
    "\n",
    "### Gradient Method\n",
    "Let the coefficient change in the direction of the gradient, and finally the function will arrive the saddle point\n",
    "$$\n",
    "x_0 = x_0 - \\eta\\frac{\\partial f}{\\partial x_0}\\\\\n",
    "x_1 = x_1 - \\eta\\frac{\\partial f}{\\partial x_1}\n",
    "$$\n",
    "\n",
    "Pay attention to the word I use is \"saddle point\", the gradient method can only reach the nearest saddle point and can not arrive at the minimum point.\n",
    "\n",
    "$\\eta$ is called *learning rate*, deciding how fast the coefficient change according to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, init_x, learn_rate = 0.01, step_num = 1000) :\n",
    "    x = init_x\n",
    "    for i in range(step_num) :\n",
    "        grad = numerical_grad(f, x)\n",
    "        x -= learn_rate * grad\n",
    "    return x\n",
    "\n",
    "# use this method to calculate the minumum of function2\n",
    "init_x = np.array([3.0, 4.0])\n",
    "final_x = grad_descent(function2, init_x)\n",
    "print(f'final value of function2 is {function2(final_x)}')\n",
    "print('the right result should be very close to 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "is in the python script in the current dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
